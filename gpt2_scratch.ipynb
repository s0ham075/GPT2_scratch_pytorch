{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEQOCaHBVIsR",
        "outputId": "34138f55-c866-4017-9ef1-565223a0cf29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "base_config = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"n_ctx\": 1024,\n",
        "    \"n_embd\": 768,\n",
        "    \"n_layer\": 12,\n",
        "    \"n_head\": 12,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "  def __init__(self,config):\n",
        "     super().__init__()\n",
        "     self.c_attn = nn.Linear(config[\"n_embd\"],3*config[\"n_embd\"])\n",
        "     self.c_proj = nn.Linear(config[\"n_embd\"],config[\"n_embd\"])\n",
        "     self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "     self.n_head = config[\"n_head\"]\n",
        "     self.n_embd = config[\"n_embd\"]\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    qkv = self.c_attn(x)\n",
        "    q,k,v = torch.split(qkv,self.n_embd,dim=2) #each will have dim B,T,C\n",
        "\n",
        "    #now for these dim becomes B,n_head,T, head_dim\n",
        "    q = q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "    k = k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "    v = v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "    attn = torch.matmul(q,k.transpose(-2,-1))/self.n_head**0.5\n",
        "\n",
        "    #attn mask\n",
        "    mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
        "    attn_scores = attn.masked_fill(mask == 0, float('-inf'))\n",
        "    attn_scores = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    out = torch.matmul(attn_scores,v) # B,n_head,T,T * B,n_head,T,head_dim (T x T * T x head_dim = T x head_dim)\n",
        "    out = out.transpose(1,2).contiguous().view(B,T,C)\n",
        "    out = self.c_proj(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class MlpBlock(nn.Module):\n",
        "  def __init__(self,config=base_config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.c_fc = nn.Linear(config[\"n_embd\"],4*config[\"n_embd\"])\n",
        "    self.c_proj = nn.Linear(4*config[\"n_embd\"],config[\"n_embd\"])\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "    self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.gelu(self.c_fc(x))\n",
        "    x = self.c_proj(x)\n",
        "    return x\n",
        "\n",
        "class Transformer_Block(nn.Module):\n",
        "  def __init__(self,config=base_config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.ln_1 = nn.LayerNorm(config[\"n_embd\"])\n",
        "        self.attn = SelfAttentionBlock(config)\n",
        "        self.ln_2 = nn.LayerNorm(config[\"n_embd\"])\n",
        "        self.mlp = MlpBlock(config)\n",
        "\n",
        "  def forward(self,x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "   def __init__(self,config=base_config):\n",
        "      super().__init__()\n",
        "      self.config = config\n",
        "\n",
        "      self.transformer = nn.ModuleDict({\n",
        "          \"wte\": nn.Embedding(config[\"vocab_size\"], config[\"n_embd\"]),\n",
        "          \"wpe\": nn.Embedding(config[\"n_ctx\"], config[\"n_embd\"]),\n",
        "          \"h\": nn.ModuleList([\n",
        "              Transformer_Block(self.config)\n",
        "              for _ in range(config[\"n_layer\"])\n",
        "          ]),\n",
        "          \"ln_f\": nn.LayerNorm(config[\"n_embd\"]),\n",
        "      })\n",
        "\n",
        "      self.lm_head = nn.Linear(config[\"n_embd\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "      # weight sharing scheme\n",
        "      self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "      self.apply(self._init_weights)\n",
        "\n",
        "   def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config[\"n_layer\"]) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "   def forward(self,idx,targets=None):\n",
        "    B,T = idx.size()\n",
        "    pos = torch.arange(T, device=idx.device)\n",
        "    tok_emb = self.transformer[\"wte\"](idx)\n",
        "    pos_emb = self.transformer[\"wpe\"](pos)\n",
        "    x = tok_emb + pos_emb\n",
        "    for block in self.transformer.h:\n",
        "       x = block(x)\n",
        "    x = self.transformer.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "   def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -self.config[\"n_ctx\"]:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "VCaE8Tm4_4J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "class Dataloader:\n",
        "  def __init__(self,batch_size,block_size,device):\n",
        "    self.B = batch_size\n",
        "    self.T = block_size\n",
        "    self.device = device\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    response = requests.get(url)\n",
        "    text = response.text\n",
        "    with open(\"tiny_shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    tokens = tokenizer.encode(text)\n",
        "    tokens= torch.tensor(tokens, dtype=torch.long)\n",
        "    n = int(0.9*len(tokens))\n",
        "    self.train_data = tokens[:n].to(device)\n",
        "    self.val_data = tokens[n:].to(device)\n",
        "    self.counter = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    #to get batches in sequence\n",
        "    sample = self.dataset_tokens[self.counter:self.counter+((self.B*self.T)+1)]\n",
        "    input = sample[:-1].view(self.B,self.T)\n",
        "    target = sample[1:].view(self.B,self.T)\n",
        "    self.counter += (self.B*self.T)\n",
        "    if self.counter >= len(self.dataset_tokens):\n",
        "      self.counter = 0\n",
        "    return input,target\n",
        "\n",
        "  def get_batch(self,split):\n",
        "    # get batches randomly\n",
        "    data = self.train_data if split == 'train' else self.val_data\n",
        "    ix = torch.randint(len(data) - self.T, (self.B,))\n",
        "    x = torch.stack([data[i:i+self.T] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+self.T+1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "Y04loTRflzFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = Dataloader(16,256,device)\n",
        "model = GPT2().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),3e-4)\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters, device=device)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = train_loader.get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "#\n",
        "for epoch in range(5000):\n",
        "  if epoch % eval_interval == 0 or epoch == 5000 - 1:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "  # and using the GradScaler if there is one.\n",
        "  x,y = train_loader.get_batch(\"train\")\n",
        "  logits,loss = model(x,y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "oIwhB07qoVUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(tokenizer.decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "w6GedfpZIN8Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}